<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SRN Styler: Text-Prompted Style Transfer for Scene Representation Networks</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SRN Styler: Text-Prompted Style Transfer for Scene Representation Networks</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://stephanie-fu.github.io">Stephanie Fu</span><br>
              <span class="author-block"><a href="https://www.mit.edu">MIT</a></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://github.com/dash102/srn_styler"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/bike_2x_noembed_000000_gt_comparisons.mp4"
                type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
        The goal of <span class="dnerf">SRN Styler</span> is to perform text-prompted style transfer on Scene Representation Networks.
      </h2>
      <p>
        Note: The teaser videos wouldn't embed properly (possibly due to Chrome extension problems, according to StackOverflow :/), so <a href="https://github.com/dash102/srn_styler.github.io/tree/main/static/videos">here</a> is a link to my gt_comparison videos (or see the presentation video below for a subset of the videos).
      </p>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Neural style transfer refers to a class of deep-learning approaches for image stylization. Existing methods often decouple the desired “style” and “content” into two reference images, and target an output in image space. However, this solution falls short of human perception on two major fronts: it is limited to 2D inputs, and requires a reference image before developing an understanding of the target style. With the advent of 3D neural fields and state-of-the-art text-embedding models (such as CLIP), we explore a possible solution to these shortcomings. The goal of this project is to develop 3D scene representations that capture style given by a text condition. Specifically, we allow scene representation networks to learn from a CLIP-based loss to ultimately parameterize a stylized version of the original scene. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    
    <!-- Related Work. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Related Work</h2>
        <div class="content has-text-justified">
          <p>
            Originally proposed by Gatys et al. [1], neural style transfer began with convolutional networks that learned representations separately targeting the content and style of an image. The stylized output was then produced by jointly minimizing the distance between these two target representations and the synthesized image’s features. Since this seminal work, numerous papers have modified its approach, with CNNs replaced by models such as transformers [2] and diffusion models [3]; target applications, such as a time-of-day transfer network for architecture photography [4]; data modalities, including language [5]; and other aspects of the original solution. CLIPstyler, introduced by Kwon and Ye [8], is a particularly relevant work that breaks from the dependence of a reference style image by featurizing text conditions with the pre-trained CLIP embedding model. 
          </p>
          <p>
            More recently, researchers have shown that 3D scenes can also be stylized. Like the original work, Chiang and Tsai et al. [6] learn disentangled representations of geometry and appearance before applying style information to the implicit scene representation. This approach still depends on a reference image from which to extract style information. To loosen this constraint, Jin et al. [7] propose LASST, an algorithm that conditions on text instead of images by featurizing style prompts with a pre-trained CLIP encoder. However, their algorithm requires a scene mesh and phrases that describe the target scene, which can be infeasible in space-constrained applications and are limited to certain spatial resolutions. 
          </p>
          <p>
            Scene representation networks, introduced by Sitzmann et al. [9], implicitly store a 3D scene in a continuous function that maps world coordinates to embeddings. Like meshes, SRNs enforce 3D structure; additionally, they are more memory-efficient and can operate at higher spatial resolutions. 
          </p>
          <p>
            Our proposed method draws inspiration from LASST and CLIPstyler’s usage of text prompts (which are easier to source for arbitrary styles), and LASST’s 3D scene reconstruction pipeline to build an SRN capable of style transfer. We use the deformation network from Nerfies to allow for shape deformations when appropriate (e.g. when prompts such as "bicycle" or "truck" are included).
          </p>
        </div>
      </div>
    </div>
    <!--/ Related Work. -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methods</h2>
        <div class="content has-text-justified">
          <p>
            We draw inspiration from the CLIP-NeRF and Nerfies methods, where a text prompt is featurized with a pre-trained CLIP encoder (from CLIP-NeRF) and shape and appearance embeddings are learned (Nerfies). See the diagram below for an overview:
          </p>
          <img src="static/images/diagram.png" />
        </div>
        <p>
          We try the following prompts, each with and without Fourier position encodings:
          <br><br>
          <ul>
            <li>A model of a blue car on a white background.</li>
            <li>A cartoon car on a white background.</li>
            <li>A pencil sketch of a car on a white background.</li>
            <li>A model of a bicycle on a white background.</li>
          </ul>
          <br><br>
          The prefix and suffix ("A model of a", "white background") were chosen to maximize style loss between the stylized prompt and a standard prompt ("A car"). This way, the model can achieve a wider range of style loss values and train more effectively. Architecture for each of the networks follows Nerfies for the deformation network, and CLIP-NeRF for the embedders.
        </p>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <p>
            <a href="https://github.com/dash102/srn_styler.github.io/tree/main/static/videos">Here</a> is a link to my result videos.
            The naming convention is: <em>(style)</em>_<em>(style_loss_weight)</em>_<em>(position_embedding)</em>_<em>(video_num)</em>_gt_comparisons.mp4. </p>

            <p>
              Below are some screenshots of results with lambda_style (CLIP loss weight) of 1x and 2x:
            </p>
            <img src="static/images/1x_diagram.png" />
            <p>
              <em>Above: Results with and without position encodings on the four prompts described in Methods, and lambda_style=1.</em>
            </p>
            <br>
            
            <img src="static/images/2x_diagram.png" />
            <p>
              <em>Above: Results with and without position encodings on the four prompts described in Methods, and lambda_style=2. In most cases, the style transfer is performed more distinctly than when lambda_style=1, yet does not collapse into entirely-blank renderings (which happens with higher weights).</em>
            </p>
            
            <p>Though we did not successfully achieve style transfer, we make a couple of observations:
            <ul>
              <li>Without position embeddings, the CLIP style loss would encourage new, hallucinated cars to be superimposed on top of the existing car reconstructions. This is possibly because the model converged on a local minimum that "satisfied" both reconstruction and style loss by building two cars. Some prominent examples can be seen with the <b>blue</b> and <b>bicycle</b> styles, where a generic blue car and a set of handlebars are superimposed on the original car reconstruction, respectively.</li>
              <li>
                In easier styles (such as <b>blue</b>), adding in position embeddings can encourage a more coherent combination of style and structure. However, the model struggles with more difficult style prompts (such as pencil sketch). This is possibly because the model needed to train longer (loss did not entirely converge), but for consistency and to realistically try more prompts, all models were trained for 175,000 steps.
              </li>
              <li>
                The style loss weight needed some tuning; at lambda_style=1 (with the default lambda_img=200, lambda_latent=1, lambda_depth=1e-3), there was very little appearance or shape influence in the final result. At lambda_style=10x, the reconstruction loss was largely ignored (e.g. the "blue" prompt resulted in a solid-blue image). See the videos labeled "1x" for examples of these failures. Since the results would already superimpose stylized cars on top of the original car shapes, increasing lambda_style from 2x would not help reconstruct a single stylized car.
              </li>
              <li>
                A partial reason why the results did not turn out as clear as CLIP-NeRF's may be because the SRN was not explicitly disentangled. Though we implemented a deformation network that took in a text-prompted shape embedding and output modified position encodings, they were input to the SRN in the same way as the text-prompted appearance embeddings. Further work on disentangling the SRN may produce cleaner results, based on the success of CLIP-NeRF's method.
              </li>
            </ul>
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/b-_T1Bstee8?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </div>
</section>

<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">References</h2>
    <div class="content has-text-centered">
      <img src="./static/images/references.png" width="30%" />
    </div>
  </div>
</div>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Thank you to <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> for the website source code.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
